#help:
help(eventsDF.distinct)

## load data
df=spark.read.format("parquet").option("inferSchema", "true").option("header", true).load(file_path)

##remove null
revenueDF = eventsDF.filter(col("ecommerce.purchase_revenue_in_usd").isNotNull())
# count distinct rows
eventsDF.distinct().count()
# sort
increaseTimestampsDF = eventsDF.sort("event_timestamp")
decreaseTimestampsDF = eventsDF.sort(col("event_timestamp").desc())
decreaseSessionsDF = eventsDF.sort(col("user_first_touch_timestamp"), col("event_timestamp"))
decreaseSessionsDF = eventsDF.sort(col("user_first_touch_timestamp").desc(), col("event_timestamp"))

df.show() #text style table output
data.show(n=1, vertical=True)
display(df) #better table output

###some characters of the spark dataframe
data.count() #rows
data.columns
data_sample=data.sample(False, fraction=0.1)
data_sort=data.sort("timestamp")

######filter
data.filter(data["Id"]==1290)
data.filter(data["Id"]==1290).count()

### describe
data.describe().show()

# correlation
data.stat.corr("column1", "column2")

#check the frequency items:
data.stat.freqItems(("columns1")).show()

#sample
data.sample(fraction=0.05, withReplacement=False)

#sql 
data.createOrReplaceTempView("tablename")
spark.sql('SELECT id, min(column1), max(column2),  stddev(colum3)\
          FROM table \
          Broup by id').show()
          
#bucket, create 10 buckets and create histgram
spark.sql('SELECT id, floor(column*10/max_column) bucket FROM table').show()
spark.sql("SELECT count(*), floor(column*10/max_column) bucket FROM table GROUP BY bucket ORDER BY bucket ").show()


#timeSeries 
spark.sql.("SELECT datetime, id, data, \
            avg(data) OVER (PARTITION BY ")
#mean
df.groupBy("id").agg({'temp':'mean'})

